{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just watched [Karpathy's BPE video](https://www.youtube.com/watch?v=zduSFxRajkE) (thanks Andrej for all the fun videos!) and got inspired. Karpathy's code is really nice (as always), and [his implementation](https://github.com/karpathy/minbpe) is obviously not meant to be fast, _but_, we **can** make the code much faster without sacrificing clarity too much. Hopefully, faster code can make experimentation (e.g. with different scores, instead of always taking the most popular pair) easier. So... here's my late night take on simple, clean, but faster BPE!\n",
    "\n",
    "For example, on my laptop, training/tokenizing the taylorswift.txt file with a vocab size of 10K takes:\n",
    "\n",
    "|              |  minbpe (Karpathy's)       |   fast_minbpe (this colab)|\n",
    "|--------------|---------------|--------------|\n",
    "|Training      |  110.10 secs  | 13.65 secs   |\n",
    "|Tokenizing    |  190.91 secs  | 0.84 secs    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 185561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swif'\n"
     ]
    }
   ],
   "source": [
    "text = open(\"taylorswift.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "print(f'Source text is of length: {len(text)}')\n",
    "print(f'First 100 chars: {repr(text[:50])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm borrowing a lot from Karpathy's code. The main differences being:\n",
    "1. We'll hold the sequence in a *linked list* (`Node` below) so that we can efficiently delete elements in the middle, and\n",
    "2. We'll compute the stats dict only _once_ at the beginning, and during training/tokenization just _update_ the relevant parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, prev=None, next=None):\n",
    "        self.id = id\n",
    "        self.prev = prev\n",
    "        self.next = next\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Build a linked list from the bytes of the text and a map from a byte to all of its occurrences.\n",
    "    tok_to_pos = {}  # This will map from a token to all of its occurrences in the linked list.\n",
    "    prev = sent = Node(None)  # For simplicity, we add a sentinel token at the start and end.\n",
    "    for t in text.encode('utf-8'):\n",
    "        node = Node(t, prev, sent)\n",
    "        tok_to_pos.setdefault(t, []).append(node)\n",
    "        prev.next = prev = node  # This is fine as assignments are evaluated left to right!\n",
    "    return sent.next, tok_to_pos\n",
    "\n",
    "def to_python_list(ll):\n",
    "    res = []\n",
    "    while ll.id is not None:\n",
    "        res.append(ll.id)\n",
    "        ll = ll.next\n",
    "    return res\n",
    "\n",
    "ll, tok_to_pos = preprocess_text(text[:10])\n",
    "assert bytes(to_python_list(ll)) == text[:10].encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 111) 1\n",
      "(111, 112) 1\n",
      "(112, 121) 1\n",
      "(121, 32) 1\n",
      "(32, 112) 1\n",
      "(112, 97) 1\n",
      "(97, 115) 1\n",
      "(115, 116) 1\n",
      "(116, 101) 1\n"
     ]
    }
   ],
   "source": [
    "def update_stats(stats, pair, delta):\n",
    "    stats[pair] = stats.get(pair, 0) + delta\n",
    "    if stats[pair] == 0:\n",
    "        del stats[pair]\n",
    "\n",
    "def init_stats(ll):\n",
    "    stats = {}\n",
    "    while ll.next.id is not None:\n",
    "        update_stats(stats, (ll.id, ll.next.id), 1)\n",
    "        ll = ll.next\n",
    "    return stats\n",
    "\n",
    "stats = init_stats(ll)\n",
    "for pair in sorted(stats, key=stats.get, reverse=True):\n",
    "    print(pair, stats[pair])\n",
    "\n",
    "assert sum(stats.values()) == len(text[:10].encode('utf-8')) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(pair, new_id, tok_to_pos, stats=None):\n",
    "    # Merge pair in our linked list (accessible through tok_to_pos) into new_id, and update the stats struct (if provided).\n",
    "    pos0_to_delete = set()\n",
    "    pos1_to_delete = set()\n",
    "\n",
    "    for pos in tok_to_pos[pair[0]]:\n",
    "        if pos in pos1_to_delete:\n",
    "            continue  # Already deleted (can happen if pair[0] == pair[1]).\n",
    "        if pos.next.id == pair[1]:\n",
    "            pos0_to_delete.add(pos)\n",
    "            pos1_to_delete.add(pos.next)\n",
    "            tok_to_pos.setdefault(new_id, []).append(pos)\n",
    "            pos.next.next.prev = pos\n",
    "            pos.next = pos.next.next\n",
    "            pos.id = new_id\n",
    "            if stats is not None:\n",
    "                # When merging (b, c) into x, so \"a b c d\" becomes \"a x d\", we need to:\n",
    "                # - Decrement (a, b), (b, c), (c, d), and\n",
    "                # - Increment (a, x), (x, d)\n",
    "                update_stats(stats, pair, -1)\n",
    "                if pos.prev.id is not None:\n",
    "                    update_stats(stats, (pos.prev.id, pair[0]), -1)\n",
    "                    update_stats(stats, (pos.prev.id, new_id), 1)\n",
    "                if pos.next.id is not None:\n",
    "                    update_stats(stats, (pair[1], pos.next.id), -1)\n",
    "                    update_stats(stats, (new_id, pos.next.id), 1)\n",
    "\n",
    "    # There's probably a better way to do this :)\n",
    "    tok_to_pos[pair[0]] = [pos for pos in tok_to_pos[pair[0]] if pos not in pos0_to_delete]\n",
    "    tok_to_pos[pair[1]] = [pos for pos in tok_to_pos[pair[1]] if pos not in pos1_to_delete]\n",
    "\n",
    "def train(text, vocab_size, verbose=False):\n",
    "    print(f'Training tokenizer on text of length {len(text):,} with vocab of size {vocab_size:,}.')\n",
    "    n_merges = vocab_size - 256\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    merge_tree = []\n",
    "    ll, tok_to_pos = preprocess_text(text)\n",
    "    stats = init_stats(ll)\n",
    "    for i in range(n_merges):\n",
    "        if not stats: break\n",
    "        top_pair = max(stats, key=stats.get)\n",
    "        new_id = len(vocab)\n",
    "        merge_tree.append((top_pair, new_id))\n",
    "        vocab[new_id] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "        if verbose:\n",
    "            print(f\"Merge {i+1}/{n_merges}: {top_pair} -> {new_id} ({vocab[new_id]}) had {stats[top_pair]} occurrences\")\n",
    "        merge(top_pair, new_id, tok_to_pos, stats)\n",
    "    \n",
    "    return merge_tree, vocab\n",
    "\n",
    "def tokenize(text, merge_tree):\n",
    "    ll, tok_to_pos = preprocess_text(text)\n",
    "    for pair, new_id in merge_tree:\n",
    "        merge(pair, new_id, tok_to_pos, None)\n",
    "    return to_python_list(ll)\n",
    "\n",
    "def detokenize(lst, vocab):\n",
    "    return b''.join((vocab[t] for t in lst)).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 512.\n",
      "Merge 1/256: (101, 32) -> 256 (b'e ') had 2981 occurrences\n",
      "Merge 2/256: (44, 32) -> 257 (b', ') had 2961 occurrences\n",
      "Merge 3/256: (100, 32) -> 258 (b'd ') had 2617 occurrences\n",
      "Merge 4/256: (46, 32) -> 259 (b'. ') had 2560 occurrences\n",
      "Merge 5/256: (114, 32) -> 260 (b'r ') had 2428 occurrences\n",
      "Merge 6/256: (50, 48) -> 261 (b'20') had 2365 occurrences\n",
      "Merge 7/256: (115, 32) -> 262 (b's ') had 2053 occurrences\n",
      "Merge 8/256: (105, 110) -> 263 (b'in') had 2006 occurrences\n",
      "Merge 9/256: (111, 110) -> 264 (b'on') had 1815 occurrences\n",
      "Merge 10/256: (114, 105) -> 265 (b'ri') had 1805 occurrences\n",
      "Merge 11/256: (116, 32) -> 266 (b't ') had 1802 occurrences\n",
      "Merge 12/256: (116, 104) -> 267 (b'th') had 1737 occurrences\n",
      "Merge 13/256: (101, 258) -> 268 (b'ed ') had 1736 occurrences\n",
      "Merge 14/256: (257, 261) -> 269 (b', 20') had 1705 occurrences\n",
      "Merge 15/256: (97, 110) -> 270 (b'an') had 1487 occurrences\n",
      "Merge 16/256: (97, 114) -> 271 (b'ar') had 1360 occurrences\n",
      "Merge 17/256: (101, 260) -> 272 (b'er ') had 1356 occurrences\n",
      "Merge 18/256: (121, 32) -> 273 (b'y ') had 1248 occurrences\n",
      "Merge 19/256: (97, 108) -> 274 (b'al') had 1164 occurrences\n",
      "Merge 20/256: (267, 256) -> 275 (b'the ') had 1142 occurrences\n",
      "Merge 21/256: (118, 268) -> 276 (b'ved ') had 1104 occurrences\n",
      "Merge 22/256: (119, 105) -> 277 (b'wi') had 1049 occurrences\n",
      "Merge 23/256: (101, 114) -> 278 (b'er') had 897 occurrences\n",
      "Merge 24/256: (264, 32) -> 279 (b'on ') had 880 occurrences\n",
      "Merge 25/256: (277, 102) -> 280 (b'wif') had 871 occurrences\n",
      "Merge 26/256: (82, 101) -> 281 (b'Re') had 870 occurrences\n",
      "Merge 27/256: (83, 280) -> 282 (b'Swif') had 867 occurrences\n",
      "Merge 28/256: (111, 260) -> 283 (b'or ') had 859 occurrences\n",
      "Merge 29/256: (99, 104) -> 284 (b'ch') had 816 occurrences\n",
      "Merge 30/256: (269, 49) -> 285 (b', 201') had 811 occurrences\n",
      "Merge 31/256: (111, 109) -> 286 (b'om') had 789 occurrences\n",
      "Merge 32/256: (98, 272) -> 287 (b'ber ') had 752 occurrences\n",
      "Merge 33/256: (32, 275) -> 288 (b' the ') had 748 occurrences\n",
      "Merge 34/256: (97, 121) -> 289 (b'ay') had 744 occurrences\n",
      "Merge 35/256: (101, 110) -> 290 (b'en') had 740 occurrences\n",
      "Merge 36/256: (111, 114) -> 291 (b'or') had 737 occurrences\n",
      "Merge 37/256: (274, 32) -> 292 (b'al ') had 705 occurrences\n",
      "Merge 38/256: (101, 109) -> 293 (b'em') had 703 occurrences\n",
      "Merge 39/256: (46, 10) -> 294 (b'.\\n') had 685 occurrences\n",
      "Merge 40/256: (265, 101) -> 295 (b'rie') had 685 occurrences\n",
      "Merge 41/256: (263, 103) -> 296 (b'ing') had 684 occurrences\n",
      "Merge 42/256: (269, 50) -> 297 (b', 202') had 673 occurrences\n",
      "Merge 43/256: (116, 105) -> 298 (b'ti') had 666 occurrences\n",
      "Merge 44/256: (289, 108) -> 299 (b'ayl') had 654 occurrences\n",
      "Merge 45/256: (34, 259) -> 300 (b'\". ') had 651 occurrences\n",
      "Merge 46/256: (108, 108) -> 301 (b'll') had 649 occurrences\n",
      "Merge 47/256: (84, 299) -> 302 (b'Tayl') had 647 occurrences\n",
      "Merge 48/256: (116, 295) -> 303 (b'trie') had 644 occurrences\n",
      "Merge 49/256: (294, 32) -> 304 (b'.\\n ') had 643 occurrences\n",
      "Merge 50/256: (116, 111) -> 305 (b'to') had 642 occurrences\n",
      "Merge 51/256: (259, 281) -> 306 (b'. Re') had 640 occurrences\n",
      "Merge 52/256: (303, 276) -> 307 (b'trieved ') had 639 occurrences\n",
      "Merge 53/256: (306, 307) -> 308 (b'. Retrieved ') had 639 occurrences\n",
      "Merge 54/256: (302, 283) -> 309 (b'Taylor ') had 611 occurrences\n",
      "Merge 55/256: (101, 115) -> 310 (b'es') had 606 occurrences\n",
      "Merge 56/256: (309, 282) -> 311 (b'Taylor Swif') had 598 occurrences\n",
      "Merge 57/256: (117, 115) -> 312 (b'us') had 561 occurrences\n",
      "Merge 58/256: (114, 286) -> 313 (b'rom') had 532 occurrences\n",
      "Merge 59/256: (293, 287) -> 314 (b'ember ') had 528 occurrences\n",
      "Merge 60/256: (41, 259) -> 315 (b'). ') had 524 occurrences\n",
      "Merge 61/256: (65, 114) -> 316 (b'Ar') had 509 occurrences\n",
      "Merge 62/256: (102, 313) -> 317 (b'from') had 503 occurrences\n",
      "Merge 63/256: (315, 34) -> 318 (b'). \"') had 499 occurrences\n",
      "Merge 64/256: (270, 258) -> 319 (b'and ') had 498 occurrences\n",
      "Merge 65/256: (114, 101) -> 320 (b're') had 495 occurrences\n",
      "Merge 66/256: (111, 117) -> 321 (b'ou') had 487 occurrences\n",
      "Merge 67/256: (111, 265) -> 322 (b'ori') had 469 occurrences\n",
      "Merge 68/256: (111, 102) -> 323 (b'of') had 466 occurrences\n",
      "Merge 69/256: (103, 263) -> 324 (b'gin') had 465 occurrences\n",
      "Merge 70/256: (296, 32) -> 325 (b'ing ') had 464 occurrences\n",
      "Merge 71/256: (93, 32) -> 326 (b'] ') had 458 occurrences\n",
      "Merge 72/256: (284, 105) -> 327 (b'chi') had 458 occurrences\n",
      "Merge 73/256: (324, 292) -> 328 (b'ginal ') had 453 occurrences\n",
      "Merge 74/256: (317, 288) -> 329 (b'from the ') had 447 occurrences\n",
      "Merge 75/256: (322, 328) -> 330 (b'original ') had 446 occurrences\n",
      "Merge 76/256: (104, 256) -> 331 (b'he ') had 440 occurrences\n",
      "Merge 77/256: (316, 327) -> 332 (b'Archi') had 440 occurrences\n",
      "Merge 78/256: (329, 330) -> 333 (b'from the original ') had 440 occurrences\n",
      "Merge 79/256: (332, 276) -> 334 (b'Archived ') had 440 occurrences\n",
      "Merge 80/256: (333, 279) -> 335 (b'from the original on ') had 439 occurrences\n",
      "Merge 81/256: (334, 335) -> 336 (b'Archived from the original on ') had 438 occurrences\n",
      "Merge 82/256: (259, 336) -> 337 (b'. Archived from the original on ') had 433 occurrences\n",
      "Merge 83/256: (97, 32) -> 338 (b'a ') had 420 occurrences\n",
      "Merge 84/256: (115, 116) -> 339 (b'st') had 409 occurrences\n",
      "Merge 85/256: (105, 99) -> 340 (b'ic') had 406 occurrences\n",
      "Merge 86/256: (46, 91) -> 341 (b'.[') had 381 occurrences\n",
      "Merge 87/256: (101, 99) -> 342 (b'ec') had 374 occurrences\n",
      "Merge 88/256: (39, 262) -> 343 (b\"'s \") had 367 occurrences\n",
      "Merge 89/256: (105, 301) -> 344 (b'ill') had 367 occurrences\n",
      "Merge 90/256: (311, 266) -> 345 (b'Taylor Swift ') had 352 occurrences\n",
      "Merge 91/256: (111, 118) -> 346 (b'ov') had 343 occurrences\n",
      "Merge 92/256: (97, 116) -> 347 (b'at') had 334 occurrences\n",
      "Merge 93/256: (97, 262) -> 348 (b'as ') had 315 occurrences\n",
      "Merge 94/256: (101, 262) -> 349 (b'es ') had 309 occurrences\n",
      "Merge 95/256: (74, 117) -> 350 (b'Ju') had 307 occurrences\n",
      "Merge 96/256: (323, 32) -> 351 (b'of ') had 306 occurrences\n",
      "Merge 97/256: (305, 32) -> 352 (b'to ') had 284 occurrences\n",
      "Merge 98/256: (117, 109) -> 353 (b'um') had 281 occurrences\n",
      "Merge 99/256: (271, 100) -> 354 (b'ard') had 277 occurrences\n",
      "Merge 100/256: (84, 331) -> 355 (b'The ') had 277 occurrences\n",
      "Merge 101/256: (263, 32) -> 356 (b'in ') had 276 occurrences\n",
      "Merge 102/256: (270, 32) -> 357 (b'an ') had 276 occurrences\n",
      "Merge 103/256: (101, 108) -> 358 (b'el') had 275 occurrences\n",
      "Merge 104/256: (297, 51) -> 359 (b', 2023') had 271 occurrences\n",
      "Merge 105/256: (271, 273) -> 360 (b'ary ') had 259 occurrences\n",
      "Merge 106/256: (267, 32) -> 361 (b'th ') had 258 occurrences\n",
      "Merge 107/256: (97, 109) -> 362 (b'am') had 257 occurrences\n",
      "Merge 108/256: (108, 273) -> 363 (b'ly ') had 250 occurrences\n",
      "Merge 109/256: (111, 112) -> 364 (b'op') had 249 occurrences\n",
      "Merge 110/256: (311, 116) -> 365 (b'Taylor Swift') had 246 occurrences\n",
      "Merge 111/256: (116, 114) -> 366 (b'tr') had 243 occurrences\n",
      "Merge 112/256: (105, 115) -> 367 (b'is') had 234 occurrences\n",
      "Merge 113/256: (104, 272) -> 368 (b'her ') had 232 occurrences\n",
      "Merge 114/256: (111, 32) -> 369 (b'o ') had 225 occurrences\n",
      "Merge 115/256: (117, 360) -> 370 (b'uary ') had 225 occurrences\n",
      "Merge 116/256: (78, 346) -> 371 (b'Nov') had 222 occurrences\n",
      "Merge 117/256: (312, 340) -> 372 (b'usic') had 221 occurrences\n",
      "Merge 118/256: (371, 314) -> 373 (b'November ') had 221 occurrences\n",
      "Merge 119/256: (101, 119) -> 374 (b'ew') had 219 occurrences\n",
      "Merge 120/256: (97, 266) -> 375 (b'at ') had 219 occurrences\n",
      "Merge 121/256: (108, 32) -> 376 (b'l ') had 218 occurrences\n",
      "Merge 122/256: (58, 32) -> 377 (b': ') had 213 occurrences\n",
      "Merge 123/256: (98, 111) -> 378 (b'bo') had 210 occurrences\n",
      "Merge 124/256: (282, 266) -> 379 (b'Swift ') had 208 occurrences\n",
      "Merge 125/256: (68, 342) -> 380 (b'Dec') had 207 occurrences\n",
      "Merge 126/256: (105, 116) -> 381 (b'it') had 206 occurrences\n",
      "Merge 127/256: (105, 103) -> 382 (b'ig') had 205 occurrences\n",
      "Merge 128/256: (66, 344) -> 383 (b'Bill') had 205 occurrences\n",
      "Merge 129/256: (49, 48) -> 384 (b'10') had 204 occurrences\n",
      "Merge 130/256: (97, 115) -> 385 (b'as') had 203 occurrences\n",
      "Merge 131/256: (264, 103) -> 386 (b'ong') had 202 occurrences\n",
      "Merge 132/256: (79, 99) -> 387 (b'Oc') had 200 occurrences\n",
      "Merge 133/256: (97, 298) -> 388 (b'ati') had 199 occurrences\n",
      "Merge 134/256: (83, 116) -> 389 (b'St') had 198 occurrences\n",
      "Merge 135/256: (305, 287) -> 390 (b'tober ') had 198 occurrences\n",
      "Merge 136/256: (387, 390) -> 391 (b'October ') had 198 occurrences\n",
      "Merge 137/256: (97, 99) -> 392 (b'ac') had 197 occurrences\n",
      "Merge 138/256: (111, 119) -> 393 (b'ow') had 196 occurrences\n",
      "Merge 139/256: (380, 314) -> 394 (b'December ') had 194 occurrences\n",
      "Merge 140/256: (383, 378) -> 395 (b'Billbo') had 191 occurrences\n",
      "Merge 141/256: (108, 101) -> 396 (b'le') had 190 occurrences\n",
      "Merge 142/256: (97, 100) -> 397 (b'ad') had 190 occurrences\n",
      "Merge 143/256: (117, 114) -> 398 (b'ur') had 188 occurrences\n",
      "Merge 144/256: (102, 283) -> 399 (b'for ') had 188 occurrences\n",
      "Merge 145/256: (32, 40) -> 400 (b' (') had 187 occurrences\n",
      "Merge 146/256: (297, 50) -> 401 (b', 2022') had 187 occurrences\n",
      "Merge 147/256: (117, 103) -> 402 (b'ug') had 185 occurrences\n",
      "Merge 148/256: (284, 32) -> 403 (b'ch ') had 184 occurrences\n",
      "Merge 149/256: (115, 266) -> 404 (b'st ') had 181 occurrences\n",
      "Merge 150/256: (321, 110) -> 405 (b'oun') had 176 occurrences\n",
      "Merge 151/256: (98, 353) -> 406 (b'bum') had 172 occurrences\n",
      "Merge 152/256: (111, 108) -> 407 (b'ol') had 171 occurrences\n",
      "Merge 153/256: (312, 266) -> 408 (b'ust ') had 171 occurrences\n",
      "Merge 154/256: (101, 98) -> 409 (b'eb') had 170 occurrences\n",
      "Merge 155/256: (77, 97) -> 410 (b'Ma') had 170 occurrences\n",
      "Merge 156/256: (350, 363) -> 411 (b'July ') had 170 occurrences\n",
      "Merge 157/256: (318, 345) -> 412 (b'). \"Taylor Swift ') had 169 occurrences\n",
      "Merge 158/256: (107, 32) -> 413 (b'k ') had 165 occurrences\n",
      "Merge 159/256: (93, 91) -> 414 (b'][') had 164 occurrences\n",
      "Merge 160/256: (278, 115) -> 415 (b'ers') had 164 occurrences\n",
      "Merge 161/256: (65, 402) -> 416 (b'Aug') had 164 occurrences\n",
      "Merge 162/256: (416, 408) -> 417 (b'August ') had 163 occurrences\n",
      "Merge 163/256: (105, 100) -> 418 (b'id') had 161 occurrences\n",
      "Merge 164/256: (297, 49) -> 419 (b', 2021') had 160 occurrences\n",
      "Merge 165/256: (109, 101) -> 420 (b'me') had 159 occurrences\n",
      "Merge 166/256: (101, 112) -> 421 (b'ep') had 156 occurrences\n",
      "Merge 167/256: (261, 49) -> 422 (b'201') had 149 occurrences\n",
      "Merge 168/256: (50, 51) -> 423 (b'23') had 145 occurrences\n",
      "Merge 169/256: (285, 50) -> 424 (b', 2012') had 144 occurrences\n",
      "Merge 170/256: (269, 261) -> 425 (b', 2020') had 140 occurrences\n",
      "Merge 171/256: (101, 271) -> 426 (b'ear') had 140 occurrences\n",
      "Merge 172/256: (73, 110) -> 427 (b'In') had 139 occurrences\n",
      "Merge 173/256: (102, 105) -> 428 (b'fi') had 139 occurrences\n",
      "Merge 174/256: (110, 256) -> 429 (b'ne ') had 139 occurrences\n",
      "Merge 175/256: (395, 354) -> 430 (b'Billboard') had 136 occurrences\n",
      "Merge 176/256: (265, 116) -> 431 (b'rit') had 134 occurrences\n",
      "Merge 177/256: (104, 105) -> 432 (b'hi') had 133 occurrences\n",
      "Merge 178/256: (304, 34) -> 433 (b'.\\n \"') had 133 occurrences\n",
      "Merge 179/256: (372, 32) -> 434 (b'usic ') had 133 occurrences\n",
      "Merge 180/256: (78, 374) -> 435 (b'New') had 131 occurrences\n",
      "Merge 181/256: (100, 105) -> 436 (b'di') had 130 occurrences\n",
      "Merge 182/256: (65, 112) -> 437 (b'Ap') had 130 occurrences\n",
      "Merge 183/256: (285, 57) -> 438 (b', 2019') had 129 occurrences\n",
      "Merge 184/256: (114, 111) -> 439 (b'ro') had 128 occurrences\n",
      "Merge 185/256: (39, 32) -> 440 (b\"' \") had 128 occurrences\n",
      "Merge 186/256: (115, 257) -> 441 (b's, ') had 127 occurrences\n",
      "Merge 187/256: (350, 429) -> 442 (b'June ') had 127 occurrences\n",
      "Merge 188/256: (50, 49) -> 443 (b'21') had 126 occurrences\n",
      "Merge 189/256: (99, 291) -> 444 (b'cor') had 126 occurrences\n",
      "Merge 190/256: (323, 288) -> 445 (b'of the ') had 126 occurrences\n",
      "Merge 191/256: (49, 57) -> 446 (b'19') had 124 occurrences\n",
      "Merge 192/256: (105, 109) -> 447 (b'im') had 123 occurrences\n",
      "Merge 193/256: (290, 32) -> 448 (b'en ') had 123 occurrences\n",
      "Merge 194/256: (409, 114) -> 449 (b'ebr') had 122 occurrences\n",
      "Merge 195/256: (290, 116) -> 450 (b'ent') had 121 occurrences\n",
      "Merge 196/256: (111, 301) -> 451 (b'oll') had 121 occurrences\n",
      "Merge 197/256: (265, 99) -> 452 (b'ric') had 120 occurrences\n",
      "Merge 198/256: (77, 271) -> 453 (b'Mar') had 120 occurrences\n",
      "Merge 199/256: (277, 361) -> 454 (b'with ') had 120 occurrences\n",
      "Merge 200/256: (44, 91) -> 455 (b',[') had 118 occurrences\n",
      "Merge 201/256: (365, 343) -> 456 (b\"Taylor Swift's \") had 118 occurrences\n",
      "Merge 202/256: (300, 430) -> 457 (b'\". Billboard') had 118 occurrences\n",
      "Merge 203/256: (70, 449) -> 458 (b'Febr') had 118 occurrences\n",
      "Merge 204/256: (458, 370) -> 459 (b'February ') had 118 occurrences\n",
      "Merge 205/256: (101, 97) -> 460 (b'ea') had 116 occurrences\n",
      "Merge 206/256: (285, 54) -> 461 (b', 2016') had 116 occurrences\n",
      "Merge 207/256: (285, 53) -> 462 (b', 2015') had 115 occurrences\n",
      "Merge 208/256: (265, 376) -> 463 (b'ril ') had 115 occurrences\n",
      "Merge 209/256: (410, 273) -> 464 (b'May ') had 115 occurrences\n",
      "Merge 210/256: (421, 116) -> 465 (b'ept') had 115 occurrences\n",
      "Merge 211/256: (437, 463) -> 466 (b'April ') had 115 occurrences\n",
      "Merge 212/256: (108, 256) -> 467 (b'le ') had 113 occurrences\n",
      "Merge 213/256: (65, 119) -> 468 (b'Aw') had 112 occurrences\n",
      "Merge 214/256: (388, 264) -> 469 (b'ation') had 112 occurrences\n",
      "Merge 215/256: (83, 465) -> 470 (b'Sept') had 112 occurrences\n",
      "Merge 216/256: (470, 314) -> 471 (b'September ') had 112 occurrences\n",
      "Merge 217/256: (114, 97) -> 472 (b'ra') had 111 occurrences\n",
      "Merge 218/256: (274, 406) -> 473 (b'album') had 111 occurrences\n",
      "Merge 219/256: (67, 104) -> 474 (b'Ch') had 110 occurrences\n",
      "Merge 220/256: (118, 256) -> 475 (b've ') had 109 occurrences\n",
      "Merge 221/256: (74, 270) -> 476 (b'Jan') had 108 occurrences\n",
      "Merge 222/256: (310, 266) -> 477 (b'est ') had 108 occurrences\n",
      "Merge 223/256: (50, 50) -> 478 (b'22') had 107 occurrences\n",
      "Merge 224/256: (476, 370) -> 479 (b'January ') had 107 occurrences\n",
      "Merge 225/256: (300, 355) -> 480 (b'\". The ') had 106 occurrences\n",
      "Merge 226/256: (359, 304) -> 481 (b', 2023.\\n ') had 106 occurrences\n",
      "Merge 227/256: (382, 104) -> 482 (b'igh') had 106 occurrences\n",
      "Merge 228/256: (405, 366) -> 483 (b'ountr') had 106 occurrences\n",
      "Merge 229/256: (49, 51) -> 484 (b'13') had 105 occurrences\n",
      "Merge 230/256: (65, 108) -> 485 (b'Al') had 105 occurrences\n",
      "Merge 231/256: (101, 116) -> 486 (b'et') had 105 occurrences\n",
      "Merge 232/256: (310, 115) -> 487 (b'ess') had 103 occurrences\n",
      "Merge 233/256: (453, 403) -> 488 (b'March ') had 103 occurrences\n",
      "Merge 234/256: (117, 116) -> 489 (b'ut') had 102 occurrences\n",
      "Merge 235/256: (119, 431) -> 490 (b'writ') had 101 occurrences\n",
      "Merge 236/256: (108, 111) -> 491 (b'lo') had 99 occurrences\n",
      "Merge 237/256: (226, 128) -> 492 (b'\\xe2\\x80') had 97 occurrences\n",
      "Merge 238/256: (48, 32) -> 493 (b'0 ') had 97 occurrences\n",
      "Merge 239/256: (271, 258) -> 494 (b'ard ') had 97 occurrences\n",
      "Merge 240/256: (115, 386) -> 495 (b'song') had 97 occurrences\n",
      "Merge 241/256: (117, 108) -> 496 (b'ul') had 96 occurrences\n",
      "Merge 242/256: (50, 52) -> 497 (b'24') had 95 occurrences\n",
      "Merge 243/256: (105, 262) -> 498 (b'is ') had 94 occurrences\n",
      "Merge 244/256: (97, 103) -> 499 (b'ag') had 93 occurrences\n",
      "Merge 245/256: (34, 32) -> 500 (b'\" ') had 93 occurrences\n",
      "Merge 246/256: (49, 56) -> 501 (b'18') had 93 occurrences\n",
      "Merge 247/256: (65, 110) -> 502 (b'An') had 93 occurrences\n",
      "Merge 248/256: (298, 99) -> 503 (b'tic') had 93 occurrences\n",
      "Merge 249/256: (102, 291) -> 504 (b'for') had 90 occurrences\n",
      "Merge 250/256: (483, 273) -> 505 (b'ountry ') had 89 occurrences\n",
      "Merge 251/256: (32, 84) -> 506 (b' T') had 88 occurrences\n",
      "Merge 252/256: (65, 420) -> 507 (b'Ame') had 88 occurrences\n",
      "Merge 253/256: (507, 452) -> 508 (b'Americ') had 88 occurrences\n",
      "Merge 254/256: (115, 296) -> 509 (b'sing') had 87 occurrences\n",
      "Merge 255/256: (49, 50) -> 510 (b'12') had 86 occurrences\n",
      "Merge 256/256: (119, 348) -> 511 (b'was ') had 86 occurrences\n"
     ]
    }
   ],
   "source": [
    "merge_tree, vocab = train(text, 512, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 300.\n",
      "Training took 0.48 seconds.\n",
      "Tokenization took 0.41 seconds.\n",
      "Tokenized text has 128451 tokens.\n",
      "Detokenize took 0.01 seconds.\n",
      "\n",
      "Training tokenizer on text of length 185,561 with vocab of size 1,000.\n",
      "Training took 1.45 seconds.\n",
      "Tokenization took 0.70 seconds.\n",
      "Tokenized text has 58341 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 13.65 seconds.\n",
      "Tokenization took 0.84 seconds.\n",
      "Tokenized text has 24372 tokens.\n",
      "Detokenize took 0.01 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def timeit(f, name):\n",
    "    start = time.time()\n",
    "    res = f()\n",
    "    print(f'{name} took {time.time() - start:.2f} seconds.')\n",
    "    return res\n",
    "\n",
    "for vocab_size in [300, 1000, 10_000]:\n",
    "    merge_tree, vocab = timeit(lambda: train(text, vocab_size), 'Training')\n",
    "    tokenized_text = timeit(lambda: tokenize(text, merge_tree), 'Tokenization')\n",
    "    print(f'Tokenized text has {len(tokenized_text)} tokens.')\n",
    "    detokenized_text = timeit(lambda: detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "    assert detokenized_text == text\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CopğŸ”y pğŸ”astğŸ”e of the ğŸ”WikipeğŸ”dia ğŸ”article ğŸ”on ğŸ”Taylor Swift, ğŸ”as of ğŸ”FebğŸ” 16ğŸ”, 2024.\n",
      "ğŸ”--ğŸ”-ğŸ”\n",
      "\n",
      "ğŸ”Main ğŸ”mğŸ”enuğŸ”\n",
      "\n",
      "ğŸ”WikipediağŸ”The FğŸ”ree ğŸ”EncğŸ”yclopedia\n",
      "ğŸ”\n",
      "ğŸ”SearchğŸ”\n",
      "ğŸ”CreğŸ”ate ğŸ”accountğŸ”\n",
      "LğŸ”ogğŸ” inğŸ”\n",
      "\n",
      "ğŸ”Personal ğŸ”toolğŸ”s\n",
      "ğŸ”ContğŸ”ents ğŸ” hğŸ”ideğŸ”\n",
      "(ğŸ”TopğŸ”)\n",
      "ğŸ”Life and ğŸ”careerğŸ”\n",
      "Toggle ğŸ”Life and ğŸ”career ğŸ”subsection\n",
      "ğŸ”ArtistryğŸ”\n",
      "Toggle ğŸ”ArtistğŸ”ry ğŸ”subsection\n",
      "ğŸ”Accolades and achievements\n",
      "ğŸ”Cultural statusğŸ”\n",
      "Toggle ğŸ”Cultural ğŸ”status ğŸ”subsection\n",
      "ğŸ”WealthğŸ”\n",
      "Toggle ğŸ”WealğŸ”th ğŸ”subsection\n",
      "ğŸ”Discography\n",
      "ğŸ”Filmography\n",
      "ğŸ”Tours\n",
      "ğŸ”See alsoğŸ”\n",
      "FğŸ”ootnotes\n",
      "ğŸ”References\n",
      "ğŸ”Toggle ğŸ”ReferğŸ”ences ğŸ”subsection\n",
      "ğŸ”External links\n",
      "ğŸ”Taylor SwiftğŸ”\n",
      "\n",
      "ğŸ”13ğŸ”6 ğŸ”lğŸ”angğŸ”uagğŸ”es\n",
      "ğŸ”ArticğŸ”le\n",
      "ğŸ”TalğŸ”k\n",
      "ğŸ”ReadğŸ”\n",
      "View ğŸ”sourğŸ”ceğŸ”\n",
      "View ğŸ”historyğŸ”\n",
      "\n",
      "ğŸ”TğŸ”ool\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "def debug(tokenized_text, vocab):\n",
    "    print('ğŸ”'.join([vocab[t].decode('utf-8') for t in tokenized_text]))\n",
    "\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
