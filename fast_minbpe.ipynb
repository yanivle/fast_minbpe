{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily inspired by Karpathy's, here's my late night take on a simple, clean, and fast BPE implementation.\n",
    "\n",
    "I'm borrowing a lot from Karpathy's code, but we'll use more efficient data-structures:\n",
    "1. We'll transform our sequence into a sequence of pairs, which we will hold in a [*Leap*](datastructures/leap.py). This will allow efficient merging.\n",
    "2. We'll calculate the pair counts only once, and hold them in a [*Multiset*](datastructures/multiset.py). This will allow efficiently finding the next pair to merge.\n",
    "\n",
    "The [*Leap*](datastructures/leap.py) is a data structure that I came up with for this (please lmk if it already has a name!). It represents an ordered list, and allows efficient iteration over items both by position and by value. It also allows for constant time insertion/appending.\n",
    "\n",
    "The [*Multiset*](datastructures/multiset.py) is functionally equivalent to the built-in `collections.Counter`. The cost of initialization and updates are a bit higher for the Multiset, but finding the top element takes constant time, making it drastically faster for our purposes (see [multiset_tests.ipynb](datastructures/multiset_tests.ipynb)).\n",
    "\n",
    "# Why is minbpe slow?\n",
    "\n",
    "If we are to perform N merges, and the length of the training sequence is L, Karpathy's original impl does (I think):\n",
    "```python\n",
    "for i in range(N):\n",
    "    calc_stats()        # O(L)\n",
    "    find_max()          # O(L)\n",
    "    do_merges()         # O(L)\n",
    "```\n",
    "For a total complexity of O(N*L) (maybe I'm neglecting some factors).\n",
    "\n",
    "# Why is fast_minbpe fast?\n",
    "\n",
    "Using our `Leap` and `Multiset`, we instead get:\n",
    "```python\n",
    "stats = calc_stats()              # O(L)\n",
    "for i in range(N):\n",
    "    find_max()                    # O(1)\n",
    "    do_merges_and_update_stats()  # O(M_i + log(L))\n",
    "```\n",
    "Where M_i denotes the actual number of merges we perform at the ith iteration. Note that M_1+M_2+...+M_n <= L - 1, so the overall complexity of evertyhing (again neglecting logarithmic factors) is O(L)!\n",
    "\n",
    "We'll unfortunately have to give up some lovely code from Karpathy's implementation, such as:\n",
    "```python\n",
    "pair = max(stats, key=stats.get)\n",
    "```\n",
    "That said, armed with the implementations of the `Leap` and the `Multiset`, the rest of our code remains concise and clean I think.\n",
    "\n",
    "Note that I only implement the functionality of Karpathy's `BasicTokenizer`.\n",
    "\n",
    "You can find some more details in [this post](https://yanivle.github.io/ai/2024/02/23/fast_minbpe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mytimeit import timeit\n",
    "from bpe import train, tokenize, detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 185,561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'\n"
     ]
    }
   ],
   "source": [
    "text = open(\"data/taylorswift.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "print(f'Source text is of length: {len(text):,}')\n",
    "print(f'First 100 chars: {repr(text[:100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 512.\n",
      "build_pairs_leap took 0.29 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Merge 1/256: (101, 32) -> 256 (b'e ') had 2981 occurrences\n",
      "Merge 2/256: (44, 32) -> 257 (b', ') had 2961 occurrences\n",
      "Merge 3/256: (100, 32) -> 258 (b'd ') had 2617 occurrences\n",
      "Merge 4/256: (46, 32) -> 259 (b'. ') had 2560 occurrences\n",
      "Merge 5/256: (114, 32) -> 260 (b'r ') had 2428 occurrences\n",
      "Merge 6/256: (50, 48) -> 261 (b'20') had 2365 occurrences\n",
      "Merge 7/256: (115, 32) -> 262 (b's ') had 2053 occurrences\n",
      "Merge 8/256: (105, 110) -> 263 (b'in') had 2006 occurrences\n",
      "Merge 9/256: (111, 110) -> 264 (b'on') had 1815 occurrences\n",
      "Merge 10/256: (114, 105) -> 265 (b'ri') had 1805 occurrences\n",
      "Merge 11/256: (116, 32) -> 266 (b't ') had 1802 occurrences\n",
      "Merge 12/256: (116, 104) -> 267 (b'th') had 1737 occurrences\n",
      "Merge 13/256: (101, 258) -> 268 (b'ed ') had 1736 occurrences\n",
      "Merge 14/256: (257, 261) -> 269 (b', 20') had 1705 occurrences\n",
      "Merge 15/256: (97, 110) -> 270 (b'an') had 1487 occurrences\n",
      "Merge 16/256: (97, 114) -> 271 (b'ar') had 1360 occurrences\n",
      "Merge 17/256: (101, 260) -> 272 (b'er ') had 1356 occurrences\n",
      "Merge 18/256: (121, 32) -> 273 (b'y ') had 1248 occurrences\n",
      "Merge 19/256: (97, 108) -> 274 (b'al') had 1164 occurrences\n",
      "Merge 20/256: (267, 256) -> 275 (b'the ') had 1142 occurrences\n",
      "Merge 21/256: (118, 268) -> 276 (b'ved ') had 1104 occurrences\n",
      "Merge 22/256: (119, 105) -> 277 (b'wi') had 1049 occurrences\n",
      "Merge 23/256: (101, 114) -> 278 (b'er') had 897 occurrences\n",
      "Merge 24/256: (264, 32) -> 279 (b'on ') had 880 occurrences\n",
      "Merge 25/256: (277, 102) -> 280 (b'wif') had 871 occurrences\n",
      "Merge 26/256: (82, 101) -> 281 (b'Re') had 870 occurrences\n",
      "Merge 27/256: (83, 280) -> 282 (b'Swif') had 867 occurrences\n",
      "Merge 28/256: (111, 260) -> 283 (b'or ') had 859 occurrences\n",
      "Merge 29/256: (99, 104) -> 284 (b'ch') had 816 occurrences\n",
      "Merge 30/256: (269, 49) -> 285 (b', 201') had 811 occurrences\n",
      "Merge 31/256: (111, 109) -> 286 (b'om') had 789 occurrences\n",
      "Merge 32/256: (98, 272) -> 287 (b'ber ') had 752 occurrences\n",
      "Merge 33/256: (32, 275) -> 288 (b' the ') had 748 occurrences\n",
      "Merge 34/256: (97, 121) -> 289 (b'ay') had 744 occurrences\n",
      "Merge 35/256: (101, 110) -> 290 (b'en') had 740 occurrences\n",
      "Merge 36/256: (111, 114) -> 291 (b'or') had 737 occurrences\n",
      "Merge 37/256: (274, 32) -> 292 (b'al ') had 705 occurrences\n",
      "Merge 38/256: (101, 109) -> 293 (b'em') had 703 occurrences\n",
      "Merge 39/256: (46, 10) -> 294 (b'.\\n') had 685 occurrences\n",
      "Merge 40/256: (265, 101) -> 295 (b'rie') had 685 occurrences\n",
      "Merge 41/256: (263, 103) -> 296 (b'ing') had 684 occurrences\n",
      "Merge 42/256: (269, 50) -> 297 (b', 202') had 673 occurrences\n",
      "Merge 43/256: (116, 105) -> 298 (b'ti') had 666 occurrences\n",
      "Merge 44/256: (289, 108) -> 299 (b'ayl') had 654 occurrences\n",
      "Merge 45/256: (34, 259) -> 300 (b'\". ') had 651 occurrences\n",
      "Merge 46/256: (108, 108) -> 301 (b'll') had 649 occurrences\n",
      "Merge 47/256: (84, 299) -> 302 (b'Tayl') had 647 occurrences\n",
      "Merge 48/256: (116, 295) -> 303 (b'trie') had 644 occurrences\n",
      "Merge 49/256: (294, 32) -> 304 (b'.\\n ') had 643 occurrences\n",
      "Merge 50/256: (116, 111) -> 305 (b'to') had 642 occurrences\n",
      "Merge 51/256: (259, 281) -> 306 (b'. Re') had 640 occurrences\n",
      "Merge 52/256: (303, 276) -> 307 (b'trieved ') had 639 occurrences\n",
      "Merge 53/256: (306, 307) -> 308 (b'. Retrieved ') had 639 occurrences\n",
      "Merge 54/256: (302, 283) -> 309 (b'Taylor ') had 611 occurrences\n",
      "Merge 55/256: (101, 115) -> 310 (b'es') had 606 occurrences\n",
      "Merge 56/256: (309, 282) -> 311 (b'Taylor Swif') had 598 occurrences\n",
      "Merge 57/256: (117, 115) -> 312 (b'us') had 561 occurrences\n",
      "Merge 58/256: (114, 286) -> 313 (b'rom') had 532 occurrences\n",
      "Merge 59/256: (293, 287) -> 314 (b'ember ') had 528 occurrences\n",
      "Merge 60/256: (41, 259) -> 315 (b'). ') had 524 occurrences\n",
      "Merge 61/256: (65, 114) -> 316 (b'Ar') had 509 occurrences\n",
      "Merge 62/256: (102, 313) -> 317 (b'from') had 503 occurrences\n",
      "Merge 63/256: (315, 34) -> 318 (b'). \"') had 499 occurrences\n",
      "Merge 64/256: (270, 258) -> 319 (b'and ') had 498 occurrences\n",
      "Merge 65/256: (114, 101) -> 320 (b're') had 495 occurrences\n",
      "Merge 66/256: (111, 117) -> 321 (b'ou') had 487 occurrences\n",
      "Merge 67/256: (111, 265) -> 322 (b'ori') had 469 occurrences\n",
      "Merge 68/256: (111, 102) -> 323 (b'of') had 466 occurrences\n",
      "Merge 69/256: (103, 263) -> 324 (b'gin') had 465 occurrences\n",
      "Merge 70/256: (296, 32) -> 325 (b'ing ') had 464 occurrences\n",
      "Merge 71/256: (284, 105) -> 326 (b'chi') had 458 occurrences\n",
      "Merge 72/256: (93, 32) -> 327 (b'] ') had 458 occurrences\n",
      "Merge 73/256: (324, 292) -> 328 (b'ginal ') had 453 occurrences\n",
      "Merge 74/256: (317, 288) -> 329 (b'from the ') had 447 occurrences\n",
      "Merge 75/256: (322, 328) -> 330 (b'original ') had 446 occurrences\n",
      "Merge 76/256: (104, 256) -> 331 (b'he ') had 440 occurrences\n",
      "Merge 77/256: (316, 326) -> 332 (b'Archi') had 440 occurrences\n",
      "Merge 78/256: (329, 330) -> 333 (b'from the original ') had 440 occurrences\n",
      "Merge 79/256: (332, 276) -> 334 (b'Archived ') had 440 occurrences\n",
      "Merge 80/256: (333, 279) -> 335 (b'from the original on ') had 439 occurrences\n",
      "Merge 81/256: (334, 335) -> 336 (b'Archived from the original on ') had 438 occurrences\n",
      "Merge 82/256: (259, 336) -> 337 (b'. Archived from the original on ') had 433 occurrences\n",
      "Merge 83/256: (97, 32) -> 338 (b'a ') had 420 occurrences\n",
      "Merge 84/256: (115, 116) -> 339 (b'st') had 409 occurrences\n",
      "Merge 85/256: (105, 99) -> 340 (b'ic') had 406 occurrences\n",
      "Merge 86/256: (46, 91) -> 341 (b'.[') had 381 occurrences\n",
      "Merge 87/256: (101, 99) -> 342 (b'ec') had 374 occurrences\n",
      "Merge 88/256: (39, 262) -> 343 (b\"'s \") had 367 occurrences\n",
      "Merge 89/256: (105, 301) -> 344 (b'ill') had 367 occurrences\n",
      "Merge 90/256: (311, 266) -> 345 (b'Taylor Swift ') had 352 occurrences\n",
      "Merge 91/256: (111, 118) -> 346 (b'ov') had 343 occurrences\n",
      "Merge 92/256: (97, 116) -> 347 (b'at') had 334 occurrences\n",
      "Merge 93/256: (97, 262) -> 348 (b'as ') had 315 occurrences\n",
      "Merge 94/256: (101, 262) -> 349 (b'es ') had 309 occurrences\n",
      "Merge 95/256: (74, 117) -> 350 (b'Ju') had 307 occurrences\n",
      "Merge 96/256: (323, 32) -> 351 (b'of ') had 306 occurrences\n",
      "Merge 97/256: (305, 32) -> 352 (b'to ') had 284 occurrences\n",
      "Merge 98/256: (117, 109) -> 353 (b'um') had 281 occurrences\n",
      "Merge 99/256: (271, 100) -> 354 (b'ard') had 277 occurrences\n",
      "Merge 100/256: (84, 331) -> 355 (b'The ') had 277 occurrences\n",
      "Merge 101/256: (270, 32) -> 356 (b'an ') had 276 occurrences\n",
      "Merge 102/256: (263, 32) -> 357 (b'in ') had 276 occurrences\n",
      "Merge 103/256: (101, 108) -> 358 (b'el') had 275 occurrences\n",
      "Merge 104/256: (297, 51) -> 359 (b', 2023') had 271 occurrences\n",
      "Merge 105/256: (271, 273) -> 360 (b'ary ') had 259 occurrences\n",
      "Merge 106/256: (267, 32) -> 361 (b'th ') had 258 occurrences\n",
      "Merge 107/256: (97, 109) -> 362 (b'am') had 257 occurrences\n",
      "Merge 108/256: (108, 273) -> 363 (b'ly ') had 250 occurrences\n",
      "Merge 109/256: (111, 112) -> 364 (b'op') had 249 occurrences\n",
      "Merge 110/256: (311, 116) -> 365 (b'Taylor Swift') had 246 occurrences\n",
      "Merge 111/256: (116, 114) -> 366 (b'tr') had 243 occurrences\n",
      "Merge 112/256: (105, 115) -> 367 (b'is') had 234 occurrences\n",
      "Merge 113/256: (104, 272) -> 368 (b'her ') had 232 occurrences\n",
      "Merge 114/256: (117, 360) -> 369 (b'uary ') had 225 occurrences\n",
      "Merge 115/256: (111, 32) -> 370 (b'o ') had 225 occurrences\n",
      "Merge 116/256: (78, 346) -> 371 (b'Nov') had 222 occurrences\n",
      "Merge 117/256: (371, 314) -> 372 (b'November ') had 221 occurrences\n",
      "Merge 118/256: (312, 340) -> 373 (b'usic') had 221 occurrences\n",
      "Merge 119/256: (101, 119) -> 374 (b'ew') had 219 occurrences\n",
      "Merge 120/256: (97, 266) -> 375 (b'at ') had 219 occurrences\n",
      "Merge 121/256: (108, 32) -> 376 (b'l ') had 218 occurrences\n",
      "Merge 122/256: (58, 32) -> 377 (b': ') had 213 occurrences\n",
      "Merge 123/256: (98, 111) -> 378 (b'bo') had 210 occurrences\n",
      "Merge 124/256: (282, 266) -> 379 (b'Swift ') had 208 occurrences\n",
      "Merge 125/256: (68, 342) -> 380 (b'Dec') had 207 occurrences\n",
      "Merge 126/256: (105, 116) -> 381 (b'it') had 206 occurrences\n",
      "Merge 127/256: (66, 344) -> 382 (b'Bill') had 205 occurrences\n",
      "Merge 128/256: (105, 103) -> 383 (b'ig') had 205 occurrences\n",
      "Merge 129/256: (49, 48) -> 384 (b'10') had 204 occurrences\n",
      "Merge 130/256: (97, 115) -> 385 (b'as') had 203 occurrences\n",
      "Merge 131/256: (264, 103) -> 386 (b'ong') had 202 occurrences\n",
      "Merge 132/256: (79, 99) -> 387 (b'Oc') had 200 occurrences\n",
      "Merge 133/256: (97, 298) -> 388 (b'ati') had 199 occurrences\n",
      "Merge 134/256: (83, 116) -> 389 (b'St') had 198 occurrences\n",
      "Merge 135/256: (387, 305) -> 390 (b'Octo') had 198 occurrences\n",
      "Merge 136/256: (390, 287) -> 391 (b'October ') had 198 occurrences\n",
      "Merge 137/256: (97, 99) -> 392 (b'ac') had 197 occurrences\n",
      "Merge 138/256: (111, 119) -> 393 (b'ow') had 196 occurrences\n",
      "Merge 139/256: (380, 314) -> 394 (b'December ') had 194 occurrences\n",
      "Merge 140/256: (382, 378) -> 395 (b'Billbo') had 191 occurrences\n",
      "Merge 141/256: (97, 100) -> 396 (b'ad') had 190 occurrences\n",
      "Merge 142/256: (108, 101) -> 397 (b'le') had 190 occurrences\n",
      "Merge 143/256: (102, 283) -> 398 (b'for ') had 188 occurrences\n",
      "Merge 144/256: (117, 114) -> 399 (b'ur') had 188 occurrences\n",
      "Merge 145/256: (32, 40) -> 400 (b' (') had 187 occurrences\n",
      "Merge 146/256: (297, 50) -> 401 (b', 2022') had 187 occurrences\n",
      "Merge 147/256: (117, 103) -> 402 (b'ug') had 185 occurrences\n",
      "Merge 148/256: (284, 32) -> 403 (b'ch ') had 184 occurrences\n",
      "Merge 149/256: (115, 266) -> 404 (b'st ') had 181 occurrences\n",
      "Merge 150/256: (321, 110) -> 405 (b'oun') had 176 occurrences\n",
      "Merge 151/256: (98, 353) -> 406 (b'bum') had 172 occurrences\n",
      "Merge 152/256: (111, 108) -> 407 (b'ol') had 171 occurrences\n",
      "Merge 153/256: (312, 266) -> 408 (b'ust ') had 171 occurrences\n",
      "Merge 154/256: (101, 98) -> 409 (b'eb') had 170 occurrences\n",
      "Merge 155/256: (350, 363) -> 410 (b'July ') had 170 occurrences\n",
      "Merge 156/256: (77, 97) -> 411 (b'Ma') had 170 occurrences\n",
      "Merge 157/256: (318, 345) -> 412 (b'). \"Taylor Swift ') had 169 occurrences\n",
      "Merge 158/256: (107, 32) -> 413 (b'k ') had 165 occurrences\n",
      "Merge 159/256: (278, 115) -> 414 (b'ers') had 164 occurrences\n",
      "Merge 160/256: (93, 91) -> 415 (b'][') had 164 occurrences\n",
      "Merge 161/256: (65, 402) -> 416 (b'Aug') had 164 occurrences\n",
      "Merge 162/256: (416, 408) -> 417 (b'August ') had 163 occurrences\n",
      "Merge 163/256: (105, 100) -> 418 (b'id') had 161 occurrences\n",
      "Merge 164/256: (297, 49) -> 419 (b', 2021') had 160 occurrences\n",
      "Merge 165/256: (109, 101) -> 420 (b'me') had 159 occurrences\n",
      "Merge 166/256: (101, 112) -> 421 (b'ep') had 156 occurrences\n",
      "Merge 167/256: (261, 49) -> 422 (b'201') had 149 occurrences\n",
      "Merge 168/256: (50, 51) -> 423 (b'23') had 145 occurrences\n",
      "Merge 169/256: (285, 50) -> 424 (b', 2012') had 144 occurrences\n",
      "Merge 170/256: (101, 271) -> 425 (b'ear') had 140 occurrences\n",
      "Merge 171/256: (269, 261) -> 426 (b', 2020') had 140 occurrences\n",
      "Merge 172/256: (102, 105) -> 427 (b'fi') had 139 occurrences\n",
      "Merge 173/256: (110, 256) -> 428 (b'ne ') had 139 occurrences\n",
      "Merge 174/256: (73, 110) -> 429 (b'In') had 139 occurrences\n",
      "Merge 175/256: (395, 354) -> 430 (b'Billboard') had 136 occurrences\n",
      "Merge 176/256: (265, 116) -> 431 (b'rit') had 134 occurrences\n",
      "Merge 177/256: (104, 105) -> 432 (b'hi') had 133 occurrences\n",
      "Merge 178/256: (373, 32) -> 433 (b'usic ') had 133 occurrences\n",
      "Merge 179/256: (304, 34) -> 434 (b'.\\n \"') had 133 occurrences\n",
      "Merge 180/256: (78, 374) -> 435 (b'New') had 131 occurrences\n",
      "Merge 181/256: (100, 105) -> 436 (b'di') had 130 occurrences\n",
      "Merge 182/256: (65, 112) -> 437 (b'Ap') had 130 occurrences\n",
      "Merge 183/256: (285, 57) -> 438 (b', 2019') had 129 occurrences\n",
      "Merge 184/256: (114, 111) -> 439 (b'ro') had 128 occurrences\n",
      "Merge 185/256: (39, 32) -> 440 (b\"' \") had 128 occurrences\n",
      "Merge 186/256: (115, 257) -> 441 (b's, ') had 127 occurrences\n",
      "Merge 187/256: (350, 428) -> 442 (b'June ') had 127 occurrences\n",
      "Merge 188/256: (99, 291) -> 443 (b'cor') had 126 occurrences\n",
      "Merge 189/256: (323, 288) -> 444 (b'of the ') had 126 occurrences\n",
      "Merge 190/256: (50, 49) -> 445 (b'21') had 126 occurrences\n",
      "Merge 191/256: (49, 57) -> 446 (b'19') had 124 occurrences\n",
      "Merge 192/256: (105, 109) -> 447 (b'im') had 123 occurrences\n",
      "Merge 193/256: (290, 32) -> 448 (b'en ') had 123 occurrences\n",
      "Merge 194/256: (409, 114) -> 449 (b'ebr') had 122 occurrences\n",
      "Merge 195/256: (290, 116) -> 450 (b'ent') had 121 occurrences\n",
      "Merge 196/256: (111, 301) -> 451 (b'oll') had 121 occurrences\n",
      "Merge 197/256: (277, 361) -> 452 (b'with ') had 120 occurrences\n",
      "Merge 198/256: (77, 271) -> 453 (b'Mar') had 120 occurrences\n",
      "Merge 199/256: (265, 99) -> 454 (b'ric') had 120 occurrences\n",
      "Merge 200/256: (365, 343) -> 455 (b\"Taylor Swift's \") had 118 occurrences\n",
      "Merge 201/256: (44, 91) -> 456 (b',[') had 118 occurrences\n",
      "Merge 202/256: (70, 449) -> 457 (b'Febr') had 118 occurrences\n",
      "Merge 203/256: (300, 430) -> 458 (b'\". Billboard') had 118 occurrences\n",
      "Merge 204/256: (457, 369) -> 459 (b'February ') had 118 occurrences\n",
      "Merge 205/256: (285, 54) -> 460 (b', 2016') had 116 occurrences\n",
      "Merge 206/256: (101, 97) -> 461 (b'ea') had 116 occurrences\n",
      "Merge 207/256: (285, 53) -> 462 (b', 2015') had 115 occurrences\n",
      "Merge 208/256: (265, 376) -> 463 (b'ril ') had 115 occurrences\n",
      "Merge 209/256: (437, 463) -> 464 (b'April ') had 115 occurrences\n",
      "Merge 210/256: (421, 116) -> 465 (b'ept') had 115 occurrences\n",
      "Merge 211/256: (411, 273) -> 466 (b'May ') had 115 occurrences\n",
      "Merge 212/256: (108, 256) -> 467 (b'le ') had 113 occurrences\n",
      "Merge 213/256: (388, 264) -> 468 (b'ation') had 112 occurrences\n",
      "Merge 214/256: (83, 465) -> 469 (b'Sept') had 112 occurrences\n",
      "Merge 215/256: (469, 314) -> 470 (b'September ') had 112 occurrences\n",
      "Merge 216/256: (65, 119) -> 471 (b'Aw') had 112 occurrences\n",
      "Merge 217/256: (114, 97) -> 472 (b'ra') had 111 occurrences\n",
      "Merge 218/256: (274, 406) -> 473 (b'album') had 111 occurrences\n",
      "Merge 219/256: (67, 104) -> 474 (b'Ch') had 110 occurrences\n",
      "Merge 220/256: (118, 256) -> 475 (b've ') had 109 occurrences\n",
      "Merge 221/256: (74, 270) -> 476 (b'Jan') had 108 occurrences\n",
      "Merge 222/256: (310, 266) -> 477 (b'est ') had 108 occurrences\n",
      "Merge 223/256: (476, 369) -> 478 (b'January ') had 107 occurrences\n",
      "Merge 224/256: (50, 50) -> 479 (b'22') had 107 occurrences\n",
      "Merge 225/256: (383, 104) -> 480 (b'igh') had 106 occurrences\n",
      "Merge 226/256: (359, 304) -> 481 (b', 2023.\\n ') had 106 occurrences\n",
      "Merge 227/256: (300, 355) -> 482 (b'\". The ') had 106 occurrences\n",
      "Merge 228/256: (405, 366) -> 483 (b'ountr') had 106 occurrences\n",
      "Merge 229/256: (101, 116) -> 484 (b'et') had 105 occurrences\n",
      "Merge 230/256: (49, 51) -> 485 (b'13') had 105 occurrences\n",
      "Merge 231/256: (65, 108) -> 486 (b'Al') had 105 occurrences\n",
      "Merge 232/256: (453, 403) -> 487 (b'March ') had 103 occurrences\n",
      "Merge 233/256: (310, 115) -> 488 (b'ess') had 103 occurrences\n",
      "Merge 234/256: (117, 116) -> 489 (b'ut') had 102 occurrences\n",
      "Merge 235/256: (119, 431) -> 490 (b'writ') had 101 occurrences\n",
      "Merge 236/256: (108, 111) -> 491 (b'lo') had 99 occurrences\n",
      "Merge 237/256: (115, 386) -> 492 (b'song') had 97 occurrences\n",
      "Merge 238/256: (48, 32) -> 493 (b'0 ') had 97 occurrences\n",
      "Merge 239/256: (271, 258) -> 494 (b'ard ') had 97 occurrences\n",
      "Merge 240/256: (226, 128) -> 495 (b'\\xe2\\x80') had 97 occurrences\n",
      "Merge 241/256: (117, 108) -> 496 (b'ul') had 96 occurrences\n",
      "Merge 242/256: (50, 52) -> 497 (b'24') had 95 occurrences\n",
      "Merge 243/256: (105, 262) -> 498 (b'is ') had 94 occurrences\n",
      "Merge 244/256: (34, 32) -> 499 (b'\" ') had 93 occurrences\n",
      "Merge 245/256: (97, 103) -> 500 (b'ag') had 93 occurrences\n",
      "Merge 246/256: (65, 110) -> 501 (b'An') had 93 occurrences\n",
      "Merge 247/256: (298, 99) -> 502 (b'tic') had 93 occurrences\n",
      "Merge 248/256: (49, 56) -> 503 (b'18') had 93 occurrences\n",
      "Merge 249/256: (102, 291) -> 504 (b'for') had 90 occurrences\n",
      "Merge 250/256: (483, 273) -> 505 (b'ountry ') had 89 occurrences\n",
      "Merge 251/256: (420, 454) -> 506 (b'meric') had 88 occurrences\n",
      "Merge 252/256: (32, 84) -> 507 (b' T') had 88 occurrences\n",
      "Merge 253/256: (65, 506) -> 508 (b'Americ') had 88 occurrences\n",
      "Merge 254/256: (115, 296) -> 509 (b'sing') had 87 occurrences\n",
      "Merge 255/256: (49, 50) -> 510 (b'12') had 86 occurrences\n",
      "Merge 256/256: (119, 348) -> 511 (b'was ') had 86 occurrences\n"
     ]
    }
   ],
   "source": [
    "merge_tree, vocab = train(text, 512, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 300.\n",
      "build_pairs_leap took 0.39 seconds.\n",
      "init_pairs_stats took 0.03 seconds.\n",
      "Training took 0.64 seconds.\n",
      "Tokenization took 0.60 seconds.\n",
      "Tokenized text has 128451 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "Training tokenizer on text of length 185,561 with vocab of size 1,000.\n",
      "build_pairs_leap took 0.49 seconds.\n",
      "init_pairs_stats took 0.01 seconds.\n",
      "Training took 1.02 seconds.\n",
      "Tokenization took 0.73 seconds.\n",
      "Tokenized text has 58337 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "build_pairs_leap took 0.40 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 1.32 seconds.\n",
      "Tokenization took 0.78 seconds.\n",
      "Tokenized text has 24302 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vocab_size in [300, 1000, 10_000]:\n",
    "    merge_tree, vocab = timeit(lambda: train(text, vocab_size), 'Training')\n",
    "    tokenized_text = timeit(lambda: tokenize(text, merge_tree), 'Tokenization')\n",
    "    print(f'Tokenized text has {len(tokenized_text)} tokens.')\n",
    "    detokenized_text = timeit(lambda: detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "    assert detokenized_text == text\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CğŸ”opğŸ”y ğŸ”pğŸ”astğŸ”e ğŸ”of the ğŸ”WikipeğŸ”dia ğŸ”article ğŸ”on ğŸ”Taylor Swift, ğŸ”as of FebğŸ” ğŸ”16ğŸ”, 2024ğŸ”.\n",
      "ğŸ”--ğŸ”-ğŸ”\n",
      "\n",
      "ğŸ”Main ğŸ”mğŸ”enuğŸ”\n",
      "\n",
      "ğŸ”WikipediağŸ”The FğŸ”ree ğŸ”EncğŸ”yclopedia\n",
      "ğŸ”\n",
      "ğŸ”Search\n",
      "ğŸ”CğŸ”reğŸ”ate ğŸ”accountğŸ”\n",
      "ğŸ”LğŸ”ogğŸ” ğŸ”inğŸ”\n",
      "\n",
      "ğŸ”Personal toolğŸ”s\n",
      "ğŸ”ContğŸ”ents ğŸ” ğŸ”hğŸ”ideğŸ”\n",
      "ğŸ”(ğŸ”TopğŸ”)\n",
      "ğŸ”Life and careerğŸ”\n",
      "Toggle ğŸ”Life and ğŸ”career ğŸ”subsection\n",
      "ğŸ”ArtistryğŸ”\n",
      "Toggle ğŸ”ArtistğŸ”ry ğŸ”subsection\n",
      "ğŸ”Accolades and achievements\n",
      "ğŸ”Cultural statusğŸ”\n",
      "Toggle ğŸ”Cultural ğŸ”status ğŸ”subsection\n",
      "ğŸ”WealthğŸ”\n",
      "Toggle ğŸ”WğŸ”ealth subsection\n",
      "ğŸ”Discography\n",
      "Filmography\n",
      "ğŸ”Tours\n",
      "ğŸ”See alsoğŸ”\n",
      "FğŸ”ootnotes\n",
      "References\n",
      "ğŸ”Toggle ğŸ”ReferencğŸ”es ğŸ”subsection\n",
      "ğŸ”External links\n",
      "Taylor Swift\n",
      "ğŸ”\n",
      "ğŸ”13ğŸ”6 ğŸ”lğŸ”angğŸ”uğŸ”agğŸ”es\n",
      "ğŸ”ArğŸ”ticğŸ”le\n",
      "ğŸ”TalğŸ”kğŸ”\n",
      "ğŸ”ReadğŸ”\n",
      "View ğŸ”sourceğŸ”\n",
      "View \n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "def debug(tokenized_text, vocab):\n",
    "    print('ğŸ”'.join([vocab[t].decode('utf-8') for t in tokenized_text]))\n",
    "\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 100,000.\n",
      "build_pairs_leap took 0.36 seconds.\n",
      "init_pairs_stats took 0.03 seconds.\n",
      "Training took 1.99 seconds.\n",
      "Tokenization took 1.10 seconds.\n",
      "Tokenized text has 1 tokens.\n",
      "Detokenize took 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "merge_tree, vocab = timeit(lambda: train(text, 100_000), 'Training')\n",
    "tokenized_text = timeit(lambda: tokenize(text, merge_tree), 'Tokenization')\n",
    "print(f'Tokenized text has {len(tokenized_text)} tokens.')\n",
    "detokenized_text = timeit(lambda: detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "assert detokenized_text == text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.99 seconds, but with a vocabulary of 100K, only 1 token remains... Let's try something longer...\n",
    "\n",
    "[The Guiness book of world records recognizes](https://www.guinnessworldrecords.com/world-records/longest-novel) Marcel Proust's \"A la recherche du temps perdu\" as the world's longest novel. Turns out it's comprised of several volumes. I found a translated version of the first volume - \"Swann's Way\" - on [the website](https://gutenberg.net.au/plusfifty-n-z.html#proust) for Project Gutenberg. Specifically [this file](https://gutenberg.net.au/ebooks03/0300511.txt). It's just over 1MB - perfect!\n",
    "Let's try training a GPT-4 sized tokenizer with a vocabulary of 100K tokens on that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 1,088,320\n",
      "First 100 chars: \"\\nProject Gutenberg Australia\\n\\nTitle:      Swann's Way\\n            (Du cÃ´tÃ© de chez Swann)\\n          \"\n"
     ]
    }
   ],
   "source": [
    "text = open(\"data/0300511.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "print(f'Source text is of length: {len(text):,}')\n",
    "print(f'First 100 chars: {repr(text[:100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "build_pairs_leap took 1.84 seconds.\n",
      "init_pairs_stats took 0.12 seconds.\n",
      "Training took 9.72 seconds.\n",
      "Tokenization took 6.27 seconds.\n",
      "Tokenized text has 86693 tokens.\n",
      "Detokenize took 0.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "merge_tree, vocab = timeit(lambda: train(text, 100_000), 'Training')\n",
    "tokenized_text = timeit(lambda: tokenize(text, merge_tree), 'Tokenization')\n",
    "print(f'Tokenized text has {len(tokenized_text)} tokens.')\n",
    "detokenized_text = timeit(lambda: detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "assert detokenized_text == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”Project Gutenberg AustraliağŸ”\n",
      "\n",
      "Title:      Swann's Way\n",
      "            (Du cÃ´tÃ© de chez Swann)\n",
      "            [Vol. 1 of Remembrance of Things Pastâ€”\n",
      "            (Ã€ la Recherche du temps perdu)]\n",
      "Author:     Marcel Proust\n",
      "            Translated from the French by C. K. Scott MoncrieffğŸ”\n",
      "ğŸ”* A Project Gutenberg of Australia ğŸ”eBook *ğŸ”\n",
      "ğŸ”eBook ğŸ”NoğŸ”.ğŸ”:  03005ğŸ”1ğŸ”1ğŸ”.ğŸ”tğŸ”xğŸ”t\n",
      "ğŸ”LğŸ”anguğŸ”ageğŸ”: ğŸ”  EnglishğŸ”\n",
      "Date ğŸ”first ğŸ”posted:          MarğŸ”ch ğŸ”20ğŸ”03ğŸ”\n",
      "Date ğŸ”most recently ğŸ”updğŸ”atedğŸ”: ğŸ”SğŸ”ept 2022ğŸ”\n",
      "\n",
      "ğŸ”ProduğŸ”ction notğŸ”es: ğŸ”WğŸ”orğŸ”ds ğŸ”in italğŸ”ics in the ğŸ”book\n",
      "ğŸ”        ğŸ”        ğŸ”  ğŸ”are ğŸ”enclosğŸ”ed by ğŸ”underscores (ğŸ”_) in this eBook\n",
      "\n",
      "ğŸ”Project Gutenberg of Australia ğŸ”eBookğŸ”s are created ğŸ”from printed editionğŸ”s\n",
      "which are in the ğŸ”public domain in ğŸ”AustraliğŸ”a, unless a ğŸ”copyğŸ”right ğŸ”notice\n",
      "ğŸ”is ğŸ”included. We do ğŸ”NOT ğŸ”keep ğŸ”any ğŸ”eBookğŸ”s in ğŸ”compliance ğŸ”with a ğŸ”particular\n",
      "ğŸ”paper ğŸ”edition.\n",
      "\n",
      "Copyright laws are ğŸ”changing ğŸ”all over the ğŸ”world. Be sure to ğŸ”check the\n",
      "copyright laws for your country before downğŸ”loading or redistğŸ”ributğŸ”ing this\n",
      "ğŸ”fiğŸ”leğŸ”.\n",
      "\n",
      "This ğŸ”eBook ğŸ”is ğŸ”made available at no ğŸ”cost and with almost no ğŸ”restrictionğŸ”s\n",
      "whatsoeverğŸ”. You may ğŸ”copy ğŸ”it, ğŸ”give ğŸ”it ğŸ”away ğŸ”or re-use ğŸ”it \n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
