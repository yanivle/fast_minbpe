{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mytimeit import timeit\n",
    "import bpe, bpe_cond_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(bpe_module, filename, vocab_size, **train_params):\n",
    "    text = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    merge_tree, vocab = timeit(lambda: bpe_module.train(text, vocab_size, **train_params), 'Training')\n",
    "    print('Top 10 tokens:', [vocab[i].decode('utf-8') for i in range(256, 256 + 10)])\n",
    "    tokenized_text = timeit(lambda: bpe_module.tokenize(text, merge_tree), 'Tokenization')\n",
    "    print(f'Tokenized text has {len(tokenized_text)} tokens ({100 * len(tokenized_text) / len(text.encode('utf-8')):.2f}% of original).')\n",
    "    detokenized_text = timeit(lambda: bpe_module.detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "    assert detokenized_text == text\n",
    "    return tokenized_text, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "build_indexed_list took 0.17 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 1.01 seconds.\n",
      "Top 10 tokens: ['e ', ', ', 'd ', '. ', 'r ', '20', 's ', 'in', 'on', 'ri']\n",
      "Tokenization took 0.44 seconds.\n",
      "Tokenized text has 24330 tokens (13.10% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=1 q=0 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.50 seconds.\n",
      "Top 10 tokens: ['e ', ', ', 'd ', '. ', 'r ', '20', 's ', 'in', 'on', 'ri']\n",
      "Tokenization took 0.42 seconds.\n",
      "Tokenized text has 24330 tokens (13.10% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.41 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', 've', 'ved', 'Sw', '. ', 'he', '. R', ', 201']\n",
      "Tokenization took 0.44 seconds.\n",
      "Tokenized text has 39534 tokens (21.28% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=4 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.44 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'ved ', 'in', 'or']\n",
      "Tokenization took 0.40 seconds.\n",
      "Tokenized text has 34036 tokens (18.32% of original).\n",
      "Detokenize took 0.01 seconds.\n",
      "\n",
      "p=5 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.57 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'in', 'or', 'ved ']\n",
      "Tokenization took 0.42 seconds.\n",
      "Tokenized text has 30647 tokens (16.50% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=2 q=1 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.23 seconds.\n",
      "Top 10 tokens: [', ', '. ', 'd ', 've', '20', 'he', 'he ', ', 20', 'Re', 'ved ']\n",
      "Tokenization took 0.38 seconds.\n",
      "Tokenized text has 46578 tokens (25.07% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=2 q=0 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.20 seconds.\n",
      "Top 10 tokens: ['20', ' 20', ' S', ', 20', ' A', ' R', 'ed', 'th', ' th', ' (']\n",
      "Tokenization took 0.37 seconds.\n",
      "Tokenized text has 47438 tokens (25.54% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=1 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.44 seconds.\n",
      "Top 10 tokens: [', ', '. ', 'd ', '20', 've', 'he', 'he ', ', 20', 's ', 'r ']\n",
      "Tokenization took 0.43 seconds.\n",
      "Tokenized text has 32658 tokens (17.58% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=0 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.38 seconds.\n",
      "Top 10 tokens: ['20', ' 20', ', 20', ' S', 'ed', ' A', 'er', 'th', ' th', 'in']\n",
      "Tokenization took 0.37 seconds.\n",
      "Tokenized text has 33682 tokens (18.13% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10_000\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\taylorswift.txt', vocab_size)\n",
    "print()\n",
    "\n",
    "for p, q, r in [[1, 0, 0], [3, 1, 1], [4, 1, 1], [5, 1, 1], [2, 1, 0], [2, 0, 1], [3, 1, 0], [3, 0, 1]]:\n",
    "  print(f'{p=} {q=} {r=}')\n",
    "  tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\taylorswift.txt', vocab_size, p=p, q=q, r=r)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.70 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'in', 'or', 'ved ']\n",
      "Tokenization took 0.43 seconds.\n",
      "Tokenized text has 30647 tokens (16.50% of original).\n",
      "Detokenize took 0.01 seconds.\n",
      "CğŸ”opğŸ”y ğŸ”pğŸ”astğŸ”e ğŸ”of the ğŸ”WikiğŸ”pğŸ”edia ğŸ”articğŸ”le ğŸ”on ğŸ”Taylor Swift, ğŸ”as of ğŸ”FebğŸ” ğŸ”16ğŸ”, 2024ğŸ”.\n",
      "ğŸ”-ğŸ”-ğŸ”-ğŸ”\n",
      "\n",
      "ğŸ”MağŸ”in ğŸ”mğŸ”enğŸ”uğŸ”\n",
      "\n",
      "WikipediağŸ”The ğŸ”Free ğŸ”EnğŸ”cğŸ”yğŸ”cğŸ”lğŸ”opğŸ”ediağŸ”\n",
      "\n",
      "ğŸ”SğŸ”earchğŸ”\n",
      "ğŸ”CğŸ”reğŸ”ate ğŸ”accountğŸ”\n",
      "ğŸ”LğŸ”ogğŸ” ğŸ”inğŸ”\n",
      "\n",
      "Personal ğŸ”toğŸ”olğŸ”s\n",
      "ğŸ”ConğŸ”tğŸ”ents ğŸ” ğŸ”hğŸ”idğŸ”eğŸ”\n",
      "ğŸ”(ğŸ”TopğŸ”)\n",
      "ğŸ”Life and carğŸ”eğŸ”erğŸ”\n",
      "Toggle Life and carğŸ”eğŸ”er ğŸ”subsection\n",
      "Artistry\n",
      "Toggle ArtistğŸ”ry ğŸ”subsection\n",
      "Accolades and ğŸ”ağŸ”chğŸ”iğŸ”eğŸ”vements\n",
      "Cultural status\n",
      "Toggle Cultural status subsection\n",
      "ğŸ”WeğŸ”alğŸ”thğŸ”\n",
      "Toggle ğŸ”WeğŸ”alğŸ”th ğŸ”subsection\n",
      "ğŸ”DğŸ”iscographğŸ”yğŸ”\n",
      "ğŸ”FilmographğŸ”yğŸ”\n",
      "ğŸ”Tours\n",
      "ğŸ”SğŸ”eğŸ”e \n"
     ]
    }
   ],
   "source": [
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\taylorswift.txt', vocab_size, p=5, q=1, r=1)\n",
    "\n",
    "# Let's inspect our tokenized text:\n",
    "def debug(tokenized_text, vocab):\n",
    "    print('ğŸ”'.join([vocab[t].decode('utf-8') for t in tokenized_text]))\n",
    "\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "build_indexed_list took 1.11 seconds.\n",
      "init_pairs_stats took 0.14 seconds.\n",
      "Training took 9.13 seconds.\n",
      "Top 10 tokens: ['e ', 'th', 'd ', 'in', 't ', ', ', 's ', 'er', 'an', ' th']\n",
      "Tokenization took 3.14 seconds.\n",
      "Tokenized text has 86612 tokens (7.95% of original).\n",
      "Detokenize took 0.02 seconds.\n",
      "\n",
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "Training took 13.57 seconds.\n",
      "Top 10 tokens: ['e ', 'th', 'in', ', ', 'd ', 'er', 't ', 's ', 'the ', 'an']\n",
      "Tokenization took 2.86 seconds.\n",
      "Tokenized text has 105976 tokens (9.72% of original).\n",
      "Detokenize took 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\0300511.txt', 100_000)\n",
    "print()\n",
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\0300511.txt', 100_000, p=5, q=1, r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”Project GutenbergğŸ” ğŸ”AustralğŸ”iğŸ”ağŸ”\n",
      "\n",
      "ğŸ”Title:     ğŸ” ğŸ”Swann's Way\n",
      "            (DğŸ”uğŸ” ğŸ”cğŸ”Ã´ğŸ”tğŸ”Ã©ğŸ” ğŸ”de chezğŸ” ğŸ”Swann)\n",
      "            [Vol. 1 of RemembranğŸ”cğŸ”e of Things Pastâ€”ğŸ”\n",
      "ğŸ”            (Ã€ğŸ” ğŸ”la Recherche du temps perdu)]ğŸ”\n",
      "ğŸ”AğŸ”uğŸ”thor:     Marcel ProustğŸ”\n",
      "ğŸ”            Translated from the FrenchğŸ” ğŸ”by C. K. Scott MoncrieffğŸ”\n",
      "ğŸ”* ğŸ”AğŸ” ğŸ”Project Gutenberg of AustralğŸ”iğŸ”a ğŸ”eğŸ”Book *ğŸ”\n",
      "ğŸ”eğŸ”Book NoğŸ”.ğŸ”: ğŸ” ğŸ”0300511ğŸ”.ğŸ”tğŸ”xğŸ”t\n",
      "ğŸ”LğŸ”anguğŸ”agğŸ”eğŸ”:   EnglishğŸ”\n",
      "ğŸ”DğŸ”ate ğŸ”first ğŸ”posğŸ”tedğŸ”: ğŸ”        ğŸ” ğŸ”MarğŸ”chğŸ” ğŸ”2003ğŸ”\n",
      "ğŸ”DğŸ”ate ğŸ”most ğŸ”recently ğŸ”upğŸ”dğŸ”atedğŸ”: ğŸ”SğŸ”epğŸ”t ğŸ”2022ğŸ”\n",
      "\n",
      "ğŸ”Production notes: Words in ğŸ”itğŸ”alğŸ”icğŸ”s in the ğŸ”bookğŸ”\n",
      "ğŸ”                  are ğŸ”enğŸ”closğŸ”ed by ğŸ”underğŸ”sğŸ”cor\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 4,351,186 with vocab of size 100,000.\n",
      "build_indexed_list took 4.99 seconds.\n",
      "init_pairs_stats took 0.58 seconds.\n",
      "Training took 22.20 seconds.\n",
      "Top 10 tokens: ['th', 'e ', ' th', 'd ', 'an', ', ', ' the ', 't ', 'in', 'er']\n",
      "Tokenization took 11.27 seconds.\n",
      "Tokenized text has 454119 tokens (10.44% of original).\n",
      "Detokenize took 0.12 seconds.\n",
      "\n",
      "Training tokenizer on text of length 4,351,186 with vocab of size 100,000.\n",
      "Training took 30.93 seconds.\n",
      "Top 10 tokens: ['th', ' th', 'e ', 'nd', ' the ', ', ', 'and', 'in', 'of', '.\\n']\n",
      "Tokenization took 10.59 seconds.\n",
      "Tokenized text has 543110 tokens (12.48% of original).\n",
      "Detokenize took 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\bible.txt', 100_000)\n",
    "print()\n",
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\bible.txt', 100_000, p=5, q=1, r=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
