{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mytimeit import timeit\n",
    "import bpe, bpe_cond_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(bpe_module, filename, vocab_size, **train_params):\n",
    "    text = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    merge_tree, vocab = timeit(lambda: bpe_module.train(text, vocab_size, **train_params), 'Training')\n",
    "    print('Top 10 tokens:', [vocab[i].decode('utf-8') for i in range(256, 256 + 10)])\n",
    "    tokenized_text = timeit(lambda: bpe_module.tokenize(text, merge_tree), 'Tokenization')\n",
    "    print(f'Tokenized text has {len(tokenized_text)} tokens ({100 * len(tokenized_text) / len(text.encode('utf-8')):.2f}% of original).')\n",
    "    detokenized_text = timeit(lambda: bpe_module.detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "    assert detokenized_text == text\n",
    "    return tokenized_text, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "build_indexed_list took 0.17 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 1.01 seconds.\n",
      "Top 10 tokens: ['e ', ', ', 'd ', '. ', 'r ', '20', 's ', 'in', 'on', 'ri']\n",
      "Tokenization took 0.44 seconds.\n",
      "Tokenized text has 24330 tokens (13.10% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=1 q=0 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.50 seconds.\n",
      "Top 10 tokens: ['e ', ', ', 'd ', '. ', 'r ', '20', 's ', 'in', 'on', 'ri']\n",
      "Tokenization took 0.42 seconds.\n",
      "Tokenized text has 24330 tokens (13.10% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.41 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', 've', 'ved', 'Sw', '. ', 'he', '. R', ', 201']\n",
      "Tokenization took 0.44 seconds.\n",
      "Tokenized text has 39534 tokens (21.28% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=4 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.44 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'ved ', 'in', 'or']\n",
      "Tokenization took 0.40 seconds.\n",
      "Tokenized text has 34036 tokens (18.32% of original).\n",
      "Detokenize took 0.01 seconds.\n",
      "\n",
      "p=5 q=1 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.57 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'in', 'or', 'ved ']\n",
      "Tokenization took 0.42 seconds.\n",
      "Tokenized text has 30647 tokens (16.50% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=2 q=1 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.23 seconds.\n",
      "Top 10 tokens: [', ', '. ', 'd ', 've', '20', 'he', 'he ', ', 20', 'Re', 'ved ']\n",
      "Tokenization took 0.38 seconds.\n",
      "Tokenized text has 46578 tokens (25.07% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=2 q=0 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.20 seconds.\n",
      "Top 10 tokens: ['20', ' 20', ' S', ', 20', ' A', ' R', 'ed', 'th', ' th', ' (']\n",
      "Tokenization took 0.37 seconds.\n",
      "Tokenized text has 47438 tokens (25.54% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=1 r=0\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.44 seconds.\n",
      "Top 10 tokens: [', ', '. ', 'd ', '20', 've', 'he', 'he ', ', 20', 's ', 'r ']\n",
      "Tokenization took 0.43 seconds.\n",
      "Tokenized text has 32658 tokens (17.58% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "p=3 q=0 r=1\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.38 seconds.\n",
      "Top 10 tokens: ['20', ' 20', ', 20', ' S', 'ed', ' A', 'er', 'th', ' th', 'in']\n",
      "Tokenization took 0.37 seconds.\n",
      "Tokenized text has 33682 tokens (18.13% of original).\n",
      "Detokenize took 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10_000\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\taylorswift.txt', vocab_size)\n",
    "print()\n",
    "\n",
    "for p, q, r in [[1, 0, 0], [3, 1, 1], [4, 1, 1], [5, 1, 1], [2, 1, 0], [2, 0, 1], [3, 1, 0], [3, 0, 1]]:\n",
    "  print(f'{p=} {q=} {r=}')\n",
    "  tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\taylorswift.txt', vocab_size, p=p, q=q, r=r)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "Training took 1.70 seconds.\n",
      "Top 10 tokens: ['20', ', ', ', 20', '. ', 'd ', 'he', 've', 'in', 'or', 'ved ']\n",
      "Tokenization took 0.43 seconds.\n",
      "Tokenized text has 30647 tokens (16.50% of original).\n",
      "Detokenize took 0.01 seconds.\n",
      "C🍔op🍔y 🍔p🍔ast🍔e 🍔of the 🍔Wiki🍔p🍔edia 🍔artic🍔le 🍔on 🍔Taylor Swift, 🍔as of 🍔Feb🍔 🍔16🍔, 2024🍔.\n",
      "🍔-🍔-🍔-🍔\n",
      "\n",
      "🍔Ma🍔in 🍔m🍔en🍔u🍔\n",
      "\n",
      "Wikipedia🍔The 🍔Free 🍔En🍔c🍔y🍔c🍔l🍔op🍔edia🍔\n",
      "\n",
      "🍔S🍔earch🍔\n",
      "🍔C🍔re🍔ate 🍔account🍔\n",
      "🍔L🍔og🍔 🍔in🍔\n",
      "\n",
      "Personal 🍔to🍔ol🍔s\n",
      "🍔Con🍔t🍔ents 🍔 🍔h🍔id🍔e🍔\n",
      "🍔(🍔Top🍔)\n",
      "🍔Life and car🍔e🍔er🍔\n",
      "Toggle Life and car🍔e🍔er 🍔subsection\n",
      "Artistry\n",
      "Toggle Artist🍔ry 🍔subsection\n",
      "Accolades and 🍔a🍔ch🍔i🍔e🍔vements\n",
      "Cultural status\n",
      "Toggle Cultural status subsection\n",
      "🍔We🍔al🍔th🍔\n",
      "Toggle 🍔We🍔al🍔th 🍔subsection\n",
      "🍔D🍔iscograph🍔y🍔\n",
      "🍔Filmograph🍔y🍔\n",
      "🍔Tours\n",
      "🍔S🍔e🍔e \n"
     ]
    }
   ],
   "source": [
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\taylorswift.txt', vocab_size, p=5, q=1, r=1)\n",
    "\n",
    "# Let's inspect our tokenized text:\n",
    "def debug(tokenized_text, vocab):\n",
    "    print('🍔'.join([vocab[t].decode('utf-8') for t in tokenized_text]))\n",
    "\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "build_indexed_list took 1.11 seconds.\n",
      "init_pairs_stats took 0.14 seconds.\n",
      "Training took 9.13 seconds.\n",
      "Top 10 tokens: ['e ', 'th', 'd ', 'in', 't ', ', ', 's ', 'er', 'an', ' th']\n",
      "Tokenization took 3.14 seconds.\n",
      "Tokenized text has 86612 tokens (7.95% of original).\n",
      "Detokenize took 0.02 seconds.\n",
      "\n",
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "Training took 13.57 seconds.\n",
      "Top 10 tokens: ['e ', 'th', 'in', ', ', 'd ', 'er', 't ', 's ', 'the ', 'an']\n",
      "Tokenization took 2.86 seconds.\n",
      "Tokenized text has 105976 tokens (9.72% of original).\n",
      "Detokenize took 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\0300511.txt', 100_000)\n",
    "print()\n",
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\0300511.txt', 100_000, p=5, q=1, r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🍔Project Gutenberg🍔 🍔Austral🍔i🍔a🍔\n",
      "\n",
      "🍔Title:     🍔 🍔Swann's Way\n",
      "            (D🍔u🍔 🍔c🍔ô🍔t🍔é🍔 🍔de chez🍔 🍔Swann)\n",
      "            [Vol. 1 of Remembran🍔c🍔e of Things Past—🍔\n",
      "🍔            (À🍔 🍔la Recherche du temps perdu)]🍔\n",
      "🍔A🍔u🍔thor:     Marcel Proust🍔\n",
      "🍔            Translated from the French🍔 🍔by C. K. Scott Moncrieff🍔\n",
      "🍔* 🍔A🍔 🍔Project Gutenberg of Austral🍔i🍔a 🍔e🍔Book *🍔\n",
      "🍔e🍔Book No🍔.🍔: 🍔 🍔0300511🍔.🍔t🍔x🍔t\n",
      "🍔L🍔angu🍔ag🍔e🍔:   English🍔\n",
      "🍔D🍔ate 🍔first 🍔pos🍔ted🍔: 🍔        🍔 🍔Mar🍔ch🍔 🍔2003🍔\n",
      "🍔D🍔ate 🍔most 🍔recently 🍔up🍔d🍔ated🍔: 🍔S🍔ep🍔t 🍔2022🍔\n",
      "\n",
      "🍔Production notes: Words in 🍔it🍔al🍔ic🍔s in the 🍔book🍔\n",
      "🍔                  are 🍔en🍔clos🍔ed by 🍔under🍔s🍔cor\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on text of length 4,351,186 with vocab of size 100,000.\n",
      "build_indexed_list took 4.99 seconds.\n",
      "init_pairs_stats took 0.58 seconds.\n",
      "Training took 22.20 seconds.\n",
      "Top 10 tokens: ['th', 'e ', ' th', 'd ', 'an', ', ', ' the ', 't ', 'in', 'er']\n",
      "Tokenization took 11.27 seconds.\n",
      "Tokenized text has 454119 tokens (10.44% of original).\n",
      "Detokenize took 0.12 seconds.\n",
      "\n",
      "Training tokenizer on text of length 4,351,186 with vocab of size 100,000.\n",
      "Training took 30.93 seconds.\n",
      "Top 10 tokens: ['th', ' th', 'e ', 'nd', ' the ', ', ', 'and', 'in', 'of', '.\\n']\n",
      "Tokenization took 10.59 seconds.\n",
      "Tokenized text has 543110 tokens (12.48% of original).\n",
      "Detokenize took 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "tokenized_text, vocab = train_and_test(bpe, r'data\\bible.txt', 100_000)\n",
    "print()\n",
    "tokenized_text, vocab = train_and_test(bpe_cond_score, r'data\\bible.txt', 100_000, p=5, q=1, r=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
